{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM6BgQsUlv5+wCccvYsVnPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NourhanDeifSayed/Mahcien-Learning-from-scratch/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrsL6aG_pyvr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the  Heart attack dataset It is a data through which the rate and severity of heart attack is analyzed\n",
        "data=pd.read_csv('heart.csv')"
      ],
      "metadata": {
        "id": "_rp386hXrZOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "ekjom95ireN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if there are nulls in the dataset\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "9xg2Q9dTrgGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if there are duplicates\n",
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "hHxDprZ3rm8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop Duplicates\n",
        "new_data=data.drop_duplicates()"
      ],
      "metadata": {
        "id": "YQkGualxrp0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.duplicated().sum()"
      ],
      "metadata": {
        "id": "599hL32Aru4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "seyF7wzJsZ8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To visualize columns in a dataset to find a suitable scaler\n",
        "new_data.hist(figsize=(10,10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gSPBUosJr3k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "UwhkoS3RsgML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler= StandardScaler()"
      ],
      "metadata": {
        "id": "VxQaRRzAskuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify columns that express features and that express target\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']"
      ],
      "metadata": {
        "id": "rtk52Qf6xR_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features=['age'\t,'trestbps',\t'chol',\t'thalach',\t'exang',\t'oldpeak',\t'slope'\t]"
      ],
      "metadata": {
        "id": "65wYF1xbwCyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
        "numeric_features = [col for col in X.columns if col not in categorical_features]\n"
      ],
      "metadata": {
        "id": "p9yk1Ga4woac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling data using stander scaler so that the data is balanced in the same ring, so the classifiation results are better\n",
        "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n"
      ],
      "metadata": {
        "id": "tSfI3IApxr0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.hstack((np.ones((X.shape[0], 1)), X))"
      ],
      "metadata": {
        "id": "rpuF260IyH5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "75uXPRulyPEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "03jHVmbFyLsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a sigmoid function where it outputs 0 or 1 based on the value of x entered to it by exponential calculation of the inverse of x\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "rve4ot1kyTps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define Class for Logistic Regression\n",
        "class LogisticRegression():\n",
        "#function to define the basic parameters like Bias,Weight,Number of iterations ,Learning Rate\n",
        "    def __init__(self, lr=0.001, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "#function to training the machine\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "#to Calculate the predictions of line , weights and bias\n",
        "        for _ in range(self.n_iters):\n",
        "            linear_pred = np.dot(X, self.weights) + self.bias\n",
        "            predictions = sigmoid(linear_pred)\n",
        "\n",
        "            dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n",
        "            db = (1/n_samples) * np.sum(predictions-y)\n",
        "\n",
        "            self.weights = self.weights - self.lr*dw\n",
        "            self.bias = self.bias - self.lr*db\n",
        "\n",
        "#predict the output using bias and weights\n",
        "    def predict(self, X):\n",
        "        linear_pred = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = sigmoid(linear_pred)\n",
        "        class_pred = [0 if y<=0.5 else 1 for y in y_pred]\n",
        "        return class_pred"
      ],
      "metadata": {
        "id": "ybvDU7Dyy1X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define Logistic Regrssion and Learning Rate\n",
        "clf = LogisticRegression(lr=0.01)\n",
        "#train model on features of  heart attack dataset\n",
        "clf.fit(X_train,y_train)\n",
        "#predict output\n",
        "y_pred = clf.predict(X_test)\n",
        "#calculate the accuracy\n",
        "def accuracy(y_pred, y_test):\n",
        "    return np.sum(y_pred==y_test)/len(y_test)\n",
        "\n",
        "acc = accuracy(y_pred, y_test)\n",
        "print(acc*100)"
      ],
      "metadata": {
        "id": "6SPAB5qTy46h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score,recall_score,f1_score"
      ],
      "metadata": {
        "id": "ov1_-QEkzHld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the persion\n",
        "persion=precision_score(y_test,y_pred,average='macro')\n",
        "print(\"The persion for Logistic Regression Model :\",persion * 100)"
      ],
      "metadata": {
        "id": "tnUtHO4s0MIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the recall\n",
        "recall=recall_score(y_test,y_pred,average='macro')\n",
        "print(\"Recall for Logistic Regression Model \",recall*100)"
      ],
      "metadata": {
        "id": "grXm5kJQ0Try"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calcualte the F1_Score\n",
        "F1_score=f1_score(y_test,y_pred,average='macro')\n",
        "print(\"F1_score for Logistic Regression Model :\",F1_score *100)"
      ],
      "metadata": {
        "id": "SvNeaIbu0le5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is used to determine the rate of heart attack based on a set of features and data does not contain nulls, but it had 1 duplicate and does not need encoding because the data does not contain categorical columns and scaling for numerical columns using Stander scaler was made because the data had different paint columns and this would have affected the results and the logistic regression model is used in classification and it is of the type supervised learning A special type of curved functions lies in a scale from 0 to 1. This can be interpreted as possibilities of belonging to certain categories.\n",
        "\n",
        "These possibilities are then used to make decisions about the final classification. For example, if the probabilities are higher than a certain value (usually 0.5 in the case of binary classification), the sample is classified into one category, and if the probabilities are lower, it is classified into another category.\n"
      ],
      "metadata": {
        "id": "BOKyNf8y6-nI"
      }
    }
  ]
}